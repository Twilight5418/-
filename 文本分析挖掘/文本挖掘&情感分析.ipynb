{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大众点评评价情感分析~\n",
    "先上结果：\n",
    "\n",
    "| 糖水店的评论文本                             | 模型预测的情感评分 |\n",
    "| :------------------------------------------- | :----------------- |\n",
    "| '糖水味道不错，滑而不腻，赞一个，下次还会来' | 0.91               |\n",
    "| '味道一般，没啥特点'                         | 0.52               |\n",
    "| '排队老半天，环境很差，味道一般般'           | 0.05               |\n",
    "\n",
    "模型的效果还可以的样子，yeah~接下来我们好好讲讲怎么做的哈，我们通过爬虫爬取了大众点评广州8家最热门糖水店的3W条评论信息以及评分作为训练数据，前面的分析我们得知*样本很不均衡*。接下来我们的整体思路就是：文本特征处理(分词、去停用词、TF-IDF)—机器学习建模—模型评价。\n",
    "\n",
    "我们先不处理样本不均衡问题，直接建模后查看结果，接下来我们再按照两种方法处理样本不均衡，对比结果。\n",
    "\n",
    "### 数据读入和探索"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T05:39:33.727253Z",
     "start_time": "2024-06-11T05:39:33.692249Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import jieba\n",
    "import joblib\n",
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    cus_id         comment_time comment_star  \\\n",
       "0     wt92  2024-05-27 23:35:00    sml-str45   \n",
       "1       悟道  2024-05-26 18:04:00    sml-str45   \n",
       "2  重度碳水爱好者  2024-05-26 20:32:00    sml-str35   \n",
       "3   安然Evol  2024-04-27 12:34:00    sml-str50   \n",
       "4     歪蜜要来  2024-04-10 15:24:00    sml-str40   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 cus_comment  \\\n",
       "0                                                                                                                                                                                                 [ 薄荷 ] 环境 ： \\n 店面 虽小 ， 五脏俱全 ， 还是 老 店铺 的 那种 味道 风格 ， 格外 亲切 。 \\n 「 凤凰 炒奶 」 新鲜 滚烫 的 鲜牛奶 加入 一个 鸡蛋黄 ， 通过 自己 不停 的 搅拌 均匀 变成 金黄色 的 成品 。 入口 润滑 浓香 扑鼻 。 「 椰汁 西米 冰 」 醇香 椰汁 搭配 饱满 西米露 ， 夏日 解暑 佳品 。 \\n                                                         \\n \\n \\t \\t                                                         收起 评价   \n",
       "1                                                                                                            红豆 双皮奶 简直 太绝 了 ！ 奶皮 厚实 ， 入口 即 化 ， 奶香 浓郁 醇厚 。 搭配 上 香甜 的 红豆 ， 口感 丰富 ， 甜而不腻 ， 每 一口 都 让 人 陶醉 。 不愧 是 招牌 甜品 ， 真的 强烈推荐 ， 吃 过 就 会 被 它 深深 折服 。 \\n 甜品 的 种类 丰富 多样 ， 每一款 都 独具 风味 。 无论是 双皮奶 的 细腻 嫩 滑 、 姜撞奶 的 独特 口感 ， 还是 其他 甜品 的 香甜 滋味 ， 都 让 人 欲罢不能 ， 每次 有 朋友 来 这边 都 带来 试下 \\n 绝对 值得 一试 ！ \\n                                                         \\n \\n \\t \\t                                                         收起 评价   \n",
       "2  [ 薄荷 ] 环境 ： \\n 在 小道 里面 ， 门面 不 大 ， 但是 有 很多 座位 ， 稍微 排 排队 ， 翻桌 还是 挺快 的 \\n [ 樱花 ] 性价比 ： \\n 说实话 份量 挺大 的 ， 价格 不 贵 ， 就是 吃不完 哈哈哈 哈哈哈 \\n 「 红豆 双皮奶 」 这个 双皮奶 我 很 爱 ， 比英 姐家 好吃 ！ \\n 「 杏仁 炒奶 」 这个 炒奶 只能 做 热 的 ， 杏仁 味十足 ， 特别 特别 的 丝 滑 ， 很 好吃 ， 也 没有 特别 甜 ， 但 我 就是 吃 了 四五口 就 容易 腻 了 但是 是 好吃 的 \\n 「 沙湾 姜撞奶 」 姜 味道 还是 很足 的 ， 很丝滑 \\n 「 菠萝 奶冰 」 我 朋友 说 很 好喝 ， 很 像 那个 旺仔 牛奶 味道 。 \\n 总体 就是 路过 尝尝 ， 别点 多 ！ \\n                                                         \\n \\n \\t \\t                                                         收起 评价   \n",
       "3                                                                     [ 薄荷 ] 环境 ： \\n 排 小队 ， 都 是 本地人 ， 开 在 老式 居民区 楼下 ， 是 当地人 爱来 的 点 。 盲点 一个 都 没 出错 ， 非常 好吃 。 \\n \\n 口味 ： \\n 「 锅蛋 芝麻糊 」 芝麻 和 奶 的 香味 混合 着 ， 真的 要不是 吃 的 太饱 ， 可以 再 来 一碗 。 \\n 「 芒果 双皮奶 」 他家 的 点 的 更 多 的 是 红豆 双皮奶 ， 我 爱人 偏爱 芒果 ， 所以 点 了 芒果 的 ， 双皮奶 也 算是 广东 家喻户晓 的 甜品 ， 单奶 口感 十分 浓郁 ， 搭配 芒果 很 有 层次 。 \\n 「 木瓜 炖雪耳 」 清凉 美容 ， 超级 好喝 \\n                                                         \\n \\n \\t \\t                                                         收起 评价   \n",
       "4                                                                                                                                                                          [ 薄荷 ] 环境 : \\n 在 一个 小区 里面 的 ， 看来 是 酒香 不怕 巷子深 哈 。 \\n \\n [ 樱花 ] 性价比 : \\n 不贵 的 ， 碗 挺 大 ， 实在 。 \\n 「 凤凰 炒奶 」 据说 是 阿姨 们 现场 炒制 的 ， 我 坐在 外面 的 座位 也 没 怎么 看到 厨房 里面 。 \\n 「 驰名 双皮奶 」 双皮奶 碗 挺 大 的 ， 凉凉的 很 解腻 。 \\n 整体 推荐 。 \\n                                                         \\n \\n \\t \\t                                                         收起 评价   \n",
       "\n",
       "   kouwei  huanjing  fuwu  shopID  stars  year  month  weekday  hour  \\\n",
       "0       4         4     4  521698    4.5  2024      5        0    23   \n",
       "1       5         2     2  521698    4.5  2024      5        6    18   \n",
       "2       3         3     3  521698    3.5  2024      5        6    20   \n",
       "3       5         4     3  521698    5.0  2024      4        5    12   \n",
       "4       4         4     4  521698    4.0  2024      4        2    15   \n",
       "\n",
       "   comment_len  \n",
       "0          179  \n",
       "1          229  \n",
       "2          286  \n",
       "3          247  \n",
       "4          183  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cus_id</th>\n",
       "      <th>comment_time</th>\n",
       "      <th>comment_star</th>\n",
       "      <th>cus_comment</th>\n",
       "      <th>kouwei</th>\n",
       "      <th>huanjing</th>\n",
       "      <th>fuwu</th>\n",
       "      <th>shopID</th>\n",
       "      <th>stars</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>comment_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wt92</td>\n",
       "      <td>2024-05-27 23:35:00</td>\n",
       "      <td>sml-str45</td>\n",
       "      <td>[ 薄荷 ] 环境 ： \\n 店面 虽小 ， 五脏俱全 ， 还是 老 店铺 的 那种 味道 风格 ， 格外 亲切 。 \\n 「 凤凰 炒奶 」 新鲜 滚烫 的 鲜牛奶 加入 一个 鸡蛋黄 ， 通过 自己 不停 的 搅拌 均匀 变成 金黄色 的 成品 。 入口 润滑 浓香 扑鼻 。 「 椰汁 西米 冰 」 醇香 椰汁 搭配 饱满 西米露 ， 夏日 解暑 佳品 。 \\n                                                         \\n \\n \\t \\t                                                         收起 评价</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>521698</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>悟道</td>\n",
       "      <td>2024-05-26 18:04:00</td>\n",
       "      <td>sml-str45</td>\n",
       "      <td>红豆 双皮奶 简直 太绝 了 ！ 奶皮 厚实 ， 入口 即 化 ， 奶香 浓郁 醇厚 。 搭配 上 香甜 的 红豆 ， 口感 丰富 ， 甜而不腻 ， 每 一口 都 让 人 陶醉 。 不愧 是 招牌 甜品 ， 真的 强烈推荐 ， 吃 过 就 会 被 它 深深 折服 。 \\n 甜品 的 种类 丰富 多样 ， 每一款 都 独具 风味 。 无论是 双皮奶 的 细腻 嫩 滑 、 姜撞奶 的 独特 口感 ， 还是 其他 甜品 的 香甜 滋味 ， 都 让 人 欲罢不能 ， 每次 有 朋友 来 这边 都 带来 试下 \\n 绝对 值得 一试 ！ \\n                                                         \\n \\n \\t \\t                                                         收起 评价</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>521698</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>重度碳水爱好者</td>\n",
       "      <td>2024-05-26 20:32:00</td>\n",
       "      <td>sml-str35</td>\n",
       "      <td>[ 薄荷 ] 环境 ： \\n 在 小道 里面 ， 门面 不 大 ， 但是 有 很多 座位 ， 稍微 排 排队 ， 翻桌 还是 挺快 的 \\n [ 樱花 ] 性价比 ： \\n 说实话 份量 挺大 的 ， 价格 不 贵 ， 就是 吃不完 哈哈哈 哈哈哈 \\n 「 红豆 双皮奶 」 这个 双皮奶 我 很 爱 ， 比英 姐家 好吃 ！ \\n 「 杏仁 炒奶 」 这个 炒奶 只能 做 热 的 ， 杏仁 味十足 ， 特别 特别 的 丝 滑 ， 很 好吃 ， 也 没有 特别 甜 ， 但 我 就是 吃 了 四五口 就 容易 腻 了 但是 是 好吃 的 \\n 「 沙湾 姜撞奶 」 姜 味道 还是 很足 的 ， 很丝滑 \\n 「 菠萝 奶冰 」 我 朋友 说 很 好喝 ， 很 像 那个 旺仔 牛奶 味道 。 \\n 总体 就是 路过 尝尝 ， 别点 多 ！ \\n                                                         \\n \\n \\t \\t                                                         收起 评价</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>521698</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>安然Evol</td>\n",
       "      <td>2024-04-27 12:34:00</td>\n",
       "      <td>sml-str50</td>\n",
       "      <td>[ 薄荷 ] 环境 ： \\n 排 小队 ， 都 是 本地人 ， 开 在 老式 居民区 楼下 ， 是 当地人 爱来 的 点 。 盲点 一个 都 没 出错 ， 非常 好吃 。 \\n \\n 口味 ： \\n 「 锅蛋 芝麻糊 」 芝麻 和 奶 的 香味 混合 着 ， 真的 要不是 吃 的 太饱 ， 可以 再 来 一碗 。 \\n 「 芒果 双皮奶 」 他家 的 点 的 更 多 的 是 红豆 双皮奶 ， 我 爱人 偏爱 芒果 ， 所以 点 了 芒果 的 ， 双皮奶 也 算是 广东 家喻户晓 的 甜品 ， 单奶 口感 十分 浓郁 ， 搭配 芒果 很 有 层次 。 \\n 「 木瓜 炖雪耳 」 清凉 美容 ， 超级 好喝 \\n                                                         \\n \\n \\t \\t                                                         收起 评价</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>521698</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>歪蜜要来</td>\n",
       "      <td>2024-04-10 15:24:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>[ 薄荷 ] 环境 : \\n 在 一个 小区 里面 的 ， 看来 是 酒香 不怕 巷子深 哈 。 \\n \\n [ 樱花 ] 性价比 : \\n 不贵 的 ， 碗 挺 大 ， 实在 。 \\n 「 凤凰 炒奶 」 据说 是 阿姨 们 现场 炒制 的 ， 我 坐在 外面 的 座位 也 没 怎么 看到 厨房 里面 。 \\n 「 驰名 双皮奶 」 双皮奶 碗 挺 大 的 ， 凉凉的 很 解腻 。 \\n 整体 推荐 。 \\n                                                         \\n \\n \\t \\t                                                         收起 评价</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>521698</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建标签值\n",
    "\n",
    "大众点评的评分分为1-5分，1-2为差评，4-5为好评，3为中评，因此我们把1-2记为0,4-5记为1,3为中评，对我们的情感分析作用不大，丢弃掉这部分数据，但是可以作为训练语料模型的语料。我们的情感评分可以转化为标签值为1的概率值，这样我们就把情感分析问题转为文本分类问题了。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:39:33.758975Z",
     "start_time": "2024-06-11T05:39:33.753406Z"
    }
   },
   "source": [
    "#构建label值\n",
    "def zhuanhuan(score):\n",
    "    if score > 3:\n",
    "        return 1\n",
    "    elif score < 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "#特征值转换\n",
    "data['target'] = data['stars'].map(lambda x:zhuanhuan(x))\n",
    "data_model = data.dropna()"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本特征处理\n",
    "\n",
    "中文文本特征处理，需要进行中文分词，jieba分词库简单好用。接下来需要过滤停用词，网上能够搜到现成的。最后就要进行文本转向量，有词库表示法、TF-IDF、word2vec等，这篇文章作了详细介绍，推荐一波 https://zhuanlan.zhihu.com/p/44917421\n",
    "\n",
    "这里我们使用sklearn库的TF-IDF工具进行文本特征提取。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:39:33.790633Z",
     "start_time": "2024-06-11T05:39:33.767498Z"
    }
   },
   "source": [
    "#切分测试集、训练集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_model['cus_comment'], data_model['target'], random_state=3, test_size=0.25)\n",
    "\n",
    "#引入停用词\n",
    "infile = open(\"stopwords.txt\",encoding='utf-8')\n",
    "stopwords_lst = infile.readlines()\n",
    "stopwords = [x.strip() for x in stopwords_lst]\n",
    "\n",
    "#中文分词\n",
    "def fenci(train_data):\n",
    "    words_df = train_data.apply(lambda x:' '.join(jieba.cut(x)))\n",
    "    return words_df\n",
    " \n",
    "x_train[:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51                                                                                                                                                                                                                                                芒果 双皮奶 好吃 ， 正是 芒果 季节 ， 双皮奶 嫩 滑 清香 ， 香芋 炒奶 真的 爱 了 ， 奶糊 里面 有 香芋 粒粒 ， 香芋 雪球 也 好吃 。\n",
       "55      大块 的 荔浦 芋头 ， 一口 一个 太 满足 了 「 香芋 糖水 」 ， 虽然 不是 热门 产品 ， 但 这个 芋头 绵软 香甜 ， 推荐 。 \\n \\n 「 黑芝麻糊 」 很 细腻 ， 芝麻 香味 浓 ， 因为 是 夏天 去 的 有点 热 ， 加上 芝麻糊 是 热 的 ， 所以 感觉 有点 甜腻 。 \\n \\n 「 凤凰 炒奶 」 阿姨 现场 炒 的 ， 蛋 香味 、 奶 香味 混合 的 汤汁 怎么 能 不 好喝 呢 ！ \\n \\n 不过 阿姨 有 一点点 凶 呢 \\n \\n \\n \\t \\t                                                         收起 评价\n",
       "59                                                        能 吃 到 这样 传统 的 糖水 的 店 ， 现在 不太多 了 ， 而且 这种 老店 更是 少之又少 ， 可能 老街坊 才 懂得 欣赏 吧 ， 一定 要 坚持下去 啊 这里 的 特色 是 奶类 ， 这次 吃 的 是 凤凰 奶糊 ， 还是 以前 的 味道 ， 总体 感觉 会 比 某花 好 ， 不会 太甜 。 另外 要 推介 它 的 香芋 蛋花 奶 ， 也 是 绝 绝子 喔 \\n \\n \\n \\t \\t                                                         收起 评价\n",
       "47    老字号 糖水 铺 ， 但 如果 不是 朋友 带路 ， 我 都 唔 记得 了 ～ \\n 看来 还是 推广 得少 。 \\n 店铺 座位 不算 多 ， 而且 比较 逼 夹 ， 不太 友好 。 \\n 「 沙湾 姜撞奶 」 「 炼奶 B 仔 龟苓膏 」 点 了 两款 基础 款 ， 味道 还算 ok ， 能 保持 正常 水准 。 但 如果 环境 整理 一下 就 更好 了 。 \\n                                                         \\n \\n \\t \\t                                                         收起 评价\n",
       "32                                                                                                                                                                                                                                                      沙湾 姜埋奶 \\n 在 西华路 吃 的 真的 好 好吃 \\n 姜汁 够 辣   而且 水 牛奶 的 味道 非常 香 \\n 真的 是 完美 的 搭配\n",
       "Name: cus_comment, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:39:33.821933Z",
     "start_time": "2024-06-11T05:39:33.792254Z"
    }
   },
   "source": [
    "#使用TF-IDF进行文本转向量处理\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=3000, ngram_range=(1,2))\n",
    "tv.fit(x_train)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17662\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'lex', 'll', 'mon', 'null', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=3000, ngram_range=(1, 2),\n",
       "                stop_words=['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*',\n",
       "                            '+', ',', '-', '--', '.', '..', '...', '......',\n",
       "                            '...................', './', '.一', '记者', '数', '年',\n",
       "                            '月', '日', '时', '分', '秒', '/', ...])"
      ],
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=3000, ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;!&#x27;, &#x27;&quot;&#x27;, &#x27;#&#x27;, &#x27;$&#x27;, &#x27;%&#x27;, &#x27;&amp;&#x27;, &quot;&#x27;&quot;, &#x27;(&#x27;, &#x27;)&#x27;, &#x27;*&#x27;,\n",
       "                            &#x27;+&#x27;, &#x27;,&#x27;, &#x27;-&#x27;, &#x27;--&#x27;, &#x27;.&#x27;, &#x27;..&#x27;, &#x27;...&#x27;, &#x27;......&#x27;,\n",
       "                            &#x27;...................&#x27;, &#x27;./&#x27;, &#x27;.一&#x27;, &#x27;记者&#x27;, &#x27;数&#x27;, &#x27;年&#x27;,\n",
       "                            &#x27;月&#x27;, &#x27;日&#x27;, &#x27;时&#x27;, &#x27;分&#x27;, &#x27;秒&#x27;, &#x27;/&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=3000, ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;!&#x27;, &#x27;&quot;&#x27;, &#x27;#&#x27;, &#x27;$&#x27;, &#x27;%&#x27;, &#x27;&amp;&#x27;, &quot;&#x27;&quot;, &#x27;(&#x27;, &#x27;)&#x27;, &#x27;*&#x27;,\n",
       "                            &#x27;+&#x27;, &#x27;,&#x27;, &#x27;-&#x27;, &#x27;--&#x27;, &#x27;.&#x27;, &#x27;..&#x27;, &#x27;...&#x27;, &#x27;......&#x27;,\n",
       "                            &#x27;...................&#x27;, &#x27;./&#x27;, &#x27;.一&#x27;, &#x27;记者&#x27;, &#x27;数&#x27;, &#x27;年&#x27;,\n",
       "                            &#x27;月&#x27;, &#x27;日&#x27;, &#x27;时&#x27;, &#x27;分&#x27;, &#x27;秒&#x27;, &#x27;/&#x27;, ...])</pre></div></div></div></div></div>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习建模\n",
    "\n",
    "特征和标签已经准备好了，接下来就是建模了。这里我们使用文本分类的经典算法朴素贝叶斯算法，而且朴素贝叶斯算法的计算量较少。特征值是评论文本经过TF-IDF处理的向量，标签值评论的分类共两类，好评是1，差评是0。情感评分为分类器预测分类1的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:39:33.837365Z",
     "start_time": "2024-06-11T05:39:33.822978Z"
    }
   },
   "source": [
    "#计算分类效果的准确率\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(tv.transform(x_train), y_train)\n",
    "classifier.score(tv.transform(x_test), y_test)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T05:39:33.852843Z",
     "start_time": "2024-06-11T05:39:33.840091Z"
    }
   },
   "source": [
    "#计算分类器的AUC值\n",
    "y_pred = classifier.predict_proba(tv.transform(x_test))[:,1]\n",
    "roc_auc_score(y_test,y_pred)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:39:33.868372Z",
     "start_time": "2024-06-11T05:39:33.853841Z"
    }
   },
   "source": [
    "#计算一条评论文本的情感评分\n",
    "def ceshi(model,strings):\n",
    "    strings_fenci = fenci(pd.Series([strings]))\n",
    "    return float(model.predict_proba(tv.transform(strings_fenci))[:,1])"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:42:57.660829Z",
     "start_time": "2024-06-11T05:42:57.641209Z"
    }
   },
   "source": [
    "#从大众点评网找两条评论来测试一下\n",
    "test1 = '很好吃，环境好，所有员工的态度都很好，上菜快，服务也很好，味道好吃，都是用蒸馏水煮的，推荐，超好吃' #5星好评\n",
    "test2 = '糯米外皮不绵滑，豆沙馅粗躁，没有香甜味。12元一碗不值。' #1星差评\n",
    "print('好评实例的模型预测情感得分为{}\\n差评实例的模型预测情感得分为{}'.format(ceshi(classifier,test1),ceshi(classifier,test2)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好评实例的模型预测情感得分为0.9461026921771922\n",
      "差评实例的模型预测情感得分为0.9255170920279938\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，准确率和AUC值都非常不错的样子，但点评网上的实际测试中，5星好评模型预测出来了，1星差评缺预测错误。为什么呢？我们查看一下**混淆矩阵**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T05:42:59.406937Z",
     "start_time": "2024-06-11T05:42:59.389364Z"
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_predict = classifier.predict(tv.transform(x_test))\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "cm"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 0, 14]], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，**负类的预测非常不准**，433单准确预测为负类的只有15.7%，应该是由于**数据不平衡**导致的，模型的默认阈值为输出值的中位数。比如逻辑回归的输出范围为[0,1]，当某个样本的输出大于0.5就会被划分为正例，反之为反例。在数据的类别不平衡时，采用默认的分类阈值可能会导致输出全部为正例，产生虚假的高准确度，导致分类失败。\n",
    "\n",
    "处理样本不均衡问题的方法，首先可以选择调整阈值，使得模型对于较少的类别更为敏感，或者选择合适的评估标准，比如ROC或者F1，而不是准确度（accuracy）。另外一种方法就是通过采样（sampling）来调整数据的不平衡。其中欠采样抛弃了大部分正例数据，从而弱化了其影响，可能会造成偏差很大的模型，同时，数据总是宝贵的，抛弃数据是很奢侈的。另外一种是过采样，下面我们就使用过采样方法来调整。\n",
    "\n",
    "### 过采样（单纯复制）\n",
    "\n",
    "单纯的重复了反例，因此会过分强调已有的反例。如果其中部分点标记错误或者是噪音，那么错误也容易被成倍的放大。因此最大的风险就是对反例过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T05:43:01.386842Z",
     "start_time": "2024-06-11T05:43:01.371746Z"
    }
   },
   "source": [
    "data['target'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1.0    53\n",
       "0.0     5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:43:02.260686Z",
     "start_time": "2024-06-11T05:43:02.254680Z"
    }
   },
   "source": [
    "#把0类样本复制10次，构造训练集\n",
    "index_tmp = y_train==0\n",
    "y_tmp = y_train[index_tmp]\n",
    "x_tmp = x_train[index_tmp]\n",
    "x_train2 = pd.concat([x_train,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp])\n",
    "y_train2 = pd.concat([y_train,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp])"
   ],
   "outputs": [],
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:48:19.270961Z",
     "start_time": "2024-06-11T05:48:19.208798Z"
    }
   },
   "source": [
    "#使用过采样样本(简单复制)进行模型训练，并查看准确率\n",
    "clf2 = MultinomialNB()\n",
    "clf2.fit(tv.transform(x_train2), y_train2)\n",
    "y_pred2 = clf2.predict_proba(tv.transform(x_test))[:,1]\n",
    "roc_auc_score(y_test,y_pred2)\n",
    "joblib.dump(clf2, 'naive_bayes_model.pkl')\n",
    "joblib.dump(tv, 'tfidf_vectorizer.pkl')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T05:43:03.726396Z",
     "start_time": "2024-06-11T05:43:03.718266Z"
    }
   },
   "source": [
    "#查看此时的混淆矩阵\n",
    "y_predict2 = clf2.predict(tv.transform(x_test))\n",
    "cm = confusion_matrix(y_test, y_predict2)\n",
    "cm"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0],\n",
       "       [ 3, 11]], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，即使是简单粗暴的复制样本来处理样本不平衡问题，负样本的识别率大幅上升了，变为77%，满满的幸福感呀~我们自己写两句评语来看看"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T05:43:06.626331Z",
     "start_time": "2024-06-11T05:43:06.609068Z"
    }
   },
   "source": [
    "ceshi(clf2,'排队人太多，环境不好，口味一般')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3861090070426321"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出把0类别的识别出来了，太棒了~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过采样（SMOTE算法）\n",
    "\n",
    "SMOTE（Synthetic minoritye over-sampling technique,SMOTE），是在局部区域通过K-近邻生成了新的反例。相较于简单的过采样，SMOTE降低了过拟合风险，但同时运算开销加大\n",
    "\n",
    "对SMOTE感兴趣的同学可以看下这篇文章https://www.jianshu.com/p/ecbc924860af"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:40:54.028057Z",
     "start_time": "2024-06-11T05:40:53.967079Z"
    }
   },
   "source": [
    "#使用SMOTE进行样本过采样处理\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversampler=SMOTE(random_state=0)\n",
    "x_train_vec = tv.transform(x_train)\n",
    "x_resampled, y_resampled = oversampler.fit_resample(x_train_vec, y_train)"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 4, n_neighbors = 6",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[83], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m oversampler\u001B[38;5;241m=\u001B[39mSMOTE(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      4\u001B[0m x_train_vec \u001B[38;5;241m=\u001B[39m tv\u001B[38;5;241m.\u001B[39mtransform(x_train)\n\u001B[1;32m----> 5\u001B[0m x_resampled, y_resampled \u001B[38;5;241m=\u001B[39m \u001B[43moversampler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train_vec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\imblearn\\base.py:208\u001B[0m, in \u001B[0;36mBaseSampler.fit_resample\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resample the dataset.\u001B[39;00m\n\u001B[0;32m    188\u001B[0m \n\u001B[0;32m    189\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;124;03m    The corresponding label of `X_resampled`.\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m--> 208\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\imblearn\\base.py:112\u001B[0m, in \u001B[0;36mSamplerMixin.fit_resample\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    106\u001B[0m X, y, binarize_y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_X_y(X, y)\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampling_strategy_ \u001B[38;5;241m=\u001B[39m check_sampling_strategy(\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampling_strategy, y, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampling_type\n\u001B[0;32m    110\u001B[0m )\n\u001B[1;32m--> 112\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    114\u001B[0m y_ \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    115\u001B[0m     label_binarize(output[\u001B[38;5;241m1\u001B[39m], classes\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39munique(y)) \u001B[38;5;28;01mif\u001B[39;00m binarize_y \u001B[38;5;28;01melse\u001B[39;00m output[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    116\u001B[0m )\n\u001B[0;32m    118\u001B[0m X_, y_ \u001B[38;5;241m=\u001B[39m arrays_transformer\u001B[38;5;241m.\u001B[39mtransform(output[\u001B[38;5;241m0\u001B[39m], y_)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:389\u001B[0m, in \u001B[0;36mSMOTE._fit_resample\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    386\u001B[0m X_class \u001B[38;5;241m=\u001B[39m _safe_indexing(X, target_class_indices)\n\u001B[0;32m    388\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnn_k_\u001B[38;5;241m.\u001B[39mfit(X_class)\n\u001B[1;32m--> 389\u001B[0m nns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn_k_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkneighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_class\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_distance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[:, \u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m    390\u001B[0m X_new, y_new \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_samples(\n\u001B[0;32m    391\u001B[0m     X_class, y\u001B[38;5;241m.\u001B[39mdtype, class_sample, X_class, nns, n_samples, \u001B[38;5;241m1.0\u001B[39m\n\u001B[0;32m    392\u001B[0m )\n\u001B[0;32m    393\u001B[0m X_resampled\u001B[38;5;241m.\u001B[39mappend(X_new)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\neighbors\\_base.py:808\u001B[0m, in \u001B[0;36mKNeighborsMixin.kneighbors\u001B[1;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[0;32m    806\u001B[0m n_samples_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_samples_fit_\n\u001B[0;32m    807\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_neighbors \u001B[38;5;241m>\u001B[39m n_samples_fit:\n\u001B[1;32m--> 808\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    809\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected n_neighbors <= n_samples, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    810\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but n_samples = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, n_neighbors = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (n_samples_fit, n_neighbors)\n\u001B[0;32m    811\u001B[0m     )\n\u001B[0;32m    813\u001B[0m n_jobs \u001B[38;5;241m=\u001B[39m effective_n_jobs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)\n\u001B[0;32m    814\u001B[0m chunked_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Expected n_neighbors <= n_samples,  but n_samples = 4, n_neighbors = 6"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:40:54.836341Z",
     "start_time": "2024-06-11T05:40:54.822670Z"
    }
   },
   "source": [
    "#原始的样本分布\n",
    "y_train.value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1.0    39\n",
       "0.0     4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T05:40:55.659879Z",
     "start_time": "2024-06-11T05:40:55.649578Z"
    }
   },
   "source": [
    "#经过SMOTE算法过采样后的样本分布情况\n",
    "pd.Series(y_resampled).value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0.0    14920\n",
       "1.0    14920\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们经过插值，把0类数据也丰富为14923个数据了，这时候正负样本的比例为1:1，接下来我们用平衡后的数据进行训练，效果如何呢，好期待啊~"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T05:40:57.766387Z",
     "start_time": "2024-06-11T05:40:57.720198Z"
    }
   },
   "source": [
    "#使用过采样样本(SMOTE)进行模型训练，并查看准确率\n",
    "clf3 = MultinomialNB()\n",
    "clf3.fit(x_resampled, y_resampled)\n",
    "y_pred3 = clf3.predict_proba(tv.transform(x_test))[:,1]\n",
    "roc_auc_score(y_test,y_pred3)"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1607 features, but MultinomialNB is expecting 3000 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[86], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m clf3 \u001B[38;5;241m=\u001B[39m MultinomialNB()\n\u001B[0;32m      3\u001B[0m clf3\u001B[38;5;241m.\u001B[39mfit(x_resampled, y_resampled)\n\u001B[1;32m----> 4\u001B[0m y_pred3 \u001B[38;5;241m=\u001B[39m \u001B[43mclf3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[:,\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m      5\u001B[0m roc_auc_score(y_test,y_pred3)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\naive_bayes.py:144\u001B[0m, in \u001B[0;36m_BaseNB.predict_proba\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_proba\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    129\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03m    Return probability estimates for the test vector X.\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m        order, as they appear in the attribute :term:`classes_`.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_log_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\naive_bayes.py:122\u001B[0m, in \u001B[0;36m_BaseNB.predict_log_proba\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;124;03mReturn log-probability estimates for the test vector X.\u001B[39;00m\n\u001B[0;32m    108\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;124;03m    order, as they appear in the attribute :term:`classes_`.\u001B[39;00m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    121\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 122\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_X\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    123\u001B[0m jll \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_joint_log_likelihood(X)\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# normalize by P(x) = P(f_1, ..., f_n)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\naive_bayes.py:574\u001B[0m, in \u001B[0;36m_BaseDiscreteNB._check_X\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    572\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_X\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    573\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 574\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\base.py:626\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    623\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_n_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\base.py:415\u001B[0m, in \u001B[0;36mBaseEstimator._check_n_features\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    414\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_features \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_:\n\u001B[1;32m--> 415\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    416\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX has \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_features\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m features, but \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    417\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis expecting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m features as input.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    418\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: X has 1607 features, but MultinomialNB is expecting 3000 features as input."
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#查看此时的准确率\n",
    "y_predict3 = clf3.predict(tv.transform(x_test))\n",
    "cm = confusion_matrix(y_test, y_predict3)\n",
    "cm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#到网上找一条差评来测试一下情感评分的预测效果\n",
    "test3 = '糯米外皮不绵滑，豆沙馅粗躁，没有香甜味。12元一碗不值。'\n",
    "ceshi(clf3,test3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，使用SMOTE插值与简单的数据复制比起来，AUC率略有提高，实际预测效果也挺好\n",
    "\n",
    "### 模型评估测试\n",
    "\n",
    "接下来我们把3W条数据都拿来训练，数据量变多了，模型效果应该会更好"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#词向量训练\n",
    "tv2 = TfidfVectorizer(stop_words=stopwords, max_features=3000, ngram_range=(1,2))\n",
    "tv2.fit(data_model['cus_comment'])\n",
    "\n",
    "#SMOTE插值\n",
    "X_tmp = tv2.transform(data_model['cus_comment'])\n",
    "y_tmp = data_model['target']\n",
    "sm = SMOTE(random_state=0)\n",
    "X,y = sm.fit_resample(X_tmp, y_tmp)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "# 保存 TfidfVectorizer 和 朴素贝叶斯模型\n",
    "joblib.dump(tv2, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(clf, 'naive_bayes_model.pkl')\n",
    "def fenxi(strings):\n",
    "    strings_fenci = fenci(pd.Series([strings]))\n",
    "    return float(clf.predict_proba(tv2.transform(strings_fenci))[:,1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T05:39:34.320199Z",
     "start_time": "2024-06-11T05:39:34.261478Z"
    }
   },
   "source": [
    "#到网上找一条差评来测试一下\n",
    "fenxi('糯米外皮不绵滑，豆沙馅粗躁，没有香甜味。12元一碗不值。')"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1581 features, but MultinomialNB is expecting 3000 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#到网上找一条差评来测试一下\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mfenxi\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m糯米外皮不绵滑，豆沙馅粗躁，没有香甜味。12元一碗不值。\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[24], line 16\u001B[0m, in \u001B[0;36mfenxi\u001B[1;34m(strings)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfenxi\u001B[39m(strings):\n\u001B[0;32m     15\u001B[0m     strings_fenci \u001B[38;5;241m=\u001B[39m fenci(pd\u001B[38;5;241m.\u001B[39mSeries([strings]))\n\u001B[1;32m---> 16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(\u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstrings_fenci\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[:,\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\naive_bayes.py:144\u001B[0m, in \u001B[0;36m_BaseNB.predict_proba\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_proba\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    129\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03m    Return probability estimates for the test vector X.\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m        order, as they appear in the attribute :term:`classes_`.\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_log_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\naive_bayes.py:122\u001B[0m, in \u001B[0;36m_BaseNB.predict_log_proba\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;124;03mReturn log-probability estimates for the test vector X.\u001B[39;00m\n\u001B[0;32m    108\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;124;03m    order, as they appear in the attribute :term:`classes_`.\u001B[39;00m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    121\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 122\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_X\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    123\u001B[0m jll \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_joint_log_likelihood(X)\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# normalize by P(x) = P(f_1, ..., f_n)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\naive_bayes.py:574\u001B[0m, in \u001B[0;36m_BaseDiscreteNB._check_X\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    572\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_X\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    573\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 574\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\base.py:626\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    623\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_n_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\gettext\\lib\\site-packages\\sklearn\\base.py:415\u001B[0m, in \u001B[0;36mBaseEstimator._check_n_features\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    414\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_features \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_:\n\u001B[1;32m--> 415\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    416\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX has \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_features\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m features, but \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    417\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis expecting \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m features as input.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    418\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: X has 1581 features, but MultinomialNB is expecting 3000 features as input."
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只用到了简单的机器学习，就做出了不错的情感分析效果，知识的力量真是强大呀，666~\n",
    "### 后续优化方向\n",
    "\n",
    "- 使用更复杂的机器学习模型如神经网络、支持向量机等\n",
    "- 模型的调参\n",
    "- 行业词库的构建\n",
    "- 增加数据量\n",
    "- 优化情感分析的算法\n",
    "- 增加标签提取等\n",
    "- 项目部署到服务器上，更好地分享和测试模型的效果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
